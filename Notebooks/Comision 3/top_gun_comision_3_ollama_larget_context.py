# -*- coding: utf-8 -*-
"""Top Gun - Comision 3 - Ollama - Larget Context.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rVjMTM-tLBi6R9WH6gqb8Tiq_jkNMEU-
"""

!curl -fsSL https://ollama.com/install.sh | sh

!ollama --version

# Commented out IPython magic to ensure Python compatibility.
!pip install colab-xterm #https://pypi.org/project/colab-xterm/
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

!ollama pull phi3.5

!ollama run phi3.5 "Hola"

!ollama run phi3.5

!pip install gradio ollama

import gradio as gr
import ollama

def chat(message):
  response = ollama.chat(model='phi3.5', messages=[
    {
      'role': 'user',
      'content': message,
    },
  ])
  return response['message']['content']


iface = gr.Interface(fn=chat, inputs="text", outputs="text")
iface.launch()

!pip install langchain_ollama

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

template = (
    "Resumir el texto en 30 palabras: {texto}. "
)

model = OllamaLLM(model="phi3.5")


def resumir_con_ollama(chunks):
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model

    parsed_results = []

    for i, chunk in enumerate(chunks, start=1):
        response = chain.invoke(
            {"texto": chunk }
        )
        print(f"Parsed batch: {i} of {len(chunks)}")
        parsed_results.append(response)

    return "\n".join(parsed_results)

def read_chunks(text_file, size=6000):
    chunks = []

    with open(text_file, 'r', encoding='utf-8') as file:
    #with open(text_file, 'r', encoding='cp1252') as file:
        while True:
            # Leer 'size' caracteres del archivo
            chunk = file.read(size)

            # Si no hay m√°s texto que leer, terminar el bucle
            if not chunk:
                break

            # Agregar el fragmento a la lista de chunks
            chunks.append(chunk)

    return chunks

chunks = read_chunks("funes_el_memorioso.txt")
result = resumir_con_ollama(chunks)
print(result)