# -*- coding: utf-8 -*-
"""Top Gun - Comision 1 - Ollama - Long Context.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tXD7b_IXRtxz2NoXPX4eJ-IOAm_4QLtM
"""

!curl -fsSL https://ollama.com/install.sh | sh

!ollama --version

# Commented out IPython magic to ensure Python compatibility.
!pip install colab-xterm #https://pypi.org/project/colab-xterm/
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

!pip install ollama gradio

!ollama pull llama3

import gradio as gr
import ollama

def chat(message):
  try:
    response = ollama.chat(model='llama3', messages=[
      {
        'role': 'user',
        'content': message,
      },
    ])
    return response['message']['content']
  except Exception as e:
    return f'Error {e}'


iface = gr.Interface(fn=chat, inputs="text", outputs="text")
iface.launch()

!ollama run llama3 "De que color es el cielo"

!pip install langchain_ollama

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

template = (
    "Resumine este texto en 50 palabras: {texto}. "
)

model = OllamaLLM(model="llama3")


def resumir_con_ollama(chunks):
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model

    parsed_results = []

    for i, chunk in enumerate(chunks, start=1):
        response = chain.invoke(
            {"texto": chunk }
        )
        print(f"Parsed batch: {i} of {len(chunks)}")
        parsed_results.append(response)

    return "\n".join(parsed_results)

def read_chunks(text_file, size=8000):
    chunks = []


    with open(text_file, 'r', encoding='utf-8') as file:
    #with open(text_file, 'r', encoding='cp1252') as file:
        while True:
            # Leer 'size' caracteres del archivo
            chunk = file.read(size)

            # Si no hay m√°s texto que leer, terminar el bucle
            if not chunk:
                break

            # Agregar el fragmento a la lista de chunks
            chunks.append(chunk)

    return chunks

chunks = read_chunks("funes_el_memorioso.txt")
result = resumir_con_ollama(chunks)
print(result)

chunks = read_chunks("maneras-ganar-gente.txt")
result = resumir_con_ollama(chunks)
print(result)