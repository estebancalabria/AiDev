# -*- coding: utf-8 -*-
"""Top Gun - Ollama - Long Prompt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_YdNXe3Fwirz2pzA4q4SAa3lsHZ4efk
"""

!curl -fsSL https://ollama.com/install.sh | sh

!ollama --version

# Commented out IPython magic to ensure Python compatibility.
!pip install colab-xterm #https://pypi.org/project/colab-xterm/
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

!ollama pull llama3.2

!ollama run llama3.2 "Hola"

!pip install ollama gradio

import gradio as gr
import ollama

def chat(message):
  response = ollama.chat(model='llama3.2', messages=[
    {
      'role': 'user',
      'content': message,
    },
  ])
  return response['message']['content']


iface = gr.Interface(fn=chat, inputs="text", outputs="text")
iface.launch()

!pip install langchain langchain_ollama

from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

#Puedo instalar chatgpt con api key, gemini,
model = OllamaLLM(model="llama3.2")

def read_chunks(text_file, size=4000):
    chunks = []


    with open(text_file, 'r', encoding='utf-8') as file:
    #with open(text_file, 'r', encoding='cp1252') as file:
        while True:
            # Leer 'size' caracteres del archivo
            chunk = file.read(size)

            # Si no hay m√°s texto que leer, terminar el bucle
            if not chunk:
                break

            # Agregar el fragmento a la lista de chunks
            chunks.append(chunk)

    return chunks

def resumir_con_ollama(chunks):

    template = (
      "Resumine este texto en 50 palabras: {texto}. "
    )

    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model

    parsed_results = []

    for i, chunk in enumerate(chunks, start=1):
        response = chain.invoke(
            {"texto": chunk }
        )
        print(f"Parsed batch: {i} of {len(chunks)}")
        parsed_results.append(response)

    return "\n".join(parsed_results)

chunks = read_chunks("funes_el_memorioso.txt")
result = resumir_con_ollama(chunks)
print(result)